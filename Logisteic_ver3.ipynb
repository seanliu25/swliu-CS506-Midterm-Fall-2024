{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Before\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Positive Words:              word        CO\n",
      "671120      worst  7.270277\n",
      "655222      waste  6.384645\n",
      "569146      stars -5.974227\n",
      "266826      great -5.766102\n",
      "67236       awful  5.484063\n",
      "216488  excellent -5.398281\n",
      "66628       avoid  5.363987\n",
      "207074    enjoyed -5.044354\n",
      "596690   terrible  5.008610\n",
      "95133      boring  4.963620\n",
      "390173       mess  4.822302\n",
      "251475    garbage  4.723951\n",
      "495193  redeeming  4.634742\n",
      "468306     poorly  4.478857\n",
      "294801   horrible  4.413345\n",
      "Top Negative Words:                word   CO\n",
      "100287    brilliatn  0.0\n",
      "415611  nealfinally  0.0\n",
      "579191     subsuper  0.0\n",
      "579192   subsuquent  0.0\n",
      "173234    despenser  0.0\n",
      "415610    nealeddie  0.0\n",
      "415606        neal4  0.0\n",
      "92028   bodydoubled  0.0\n",
      "18816        5raven  0.0\n",
      "173231      despeja  0.0\n",
      "579201      subtect  0.0\n",
      "270938      guffamn  0.0\n",
      "270939      guffans  0.0\n",
      "579204  subteenaged  0.0\n",
      "492124   reallizing  0.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load training data\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Downsize data with Score = 5.0 to balance classes (first method)\n",
    "target_size = int(0.21 * len(data))  # Accuracy target adjustment, can increase or decrease this ratio\n",
    "score_5_downsampled = data[data['Score'] == 5.0].sample(n=target_size, random_state=42)\n",
    "other_scores = data[data['Score'] != 5.0]\n",
    "data = pd.concat([score_5_downsampled, other_scores], ignore_index=True)\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=True)\n",
    "encoded_product_user_ids = one_hot_encoder.fit_transform(data[['ProductId', 'UserId']])\n",
    "\n",
    "# Impute missing values by replacing NaNs in text columns with empty strings\n",
    "data['Text'] = data['Text'].apply(lambda x: '' if pd.isna(x) else x)\n",
    "data['Summary'] = data['Summary'].apply(lambda x: '' if pd.isna(x) else x)\n",
    "# Standardize numerical features\n",
    "data['Good'] = data['HelpfulnessNumerator']\n",
    "data['Bad'] = data['HelpfulnessDenominator'] - data['HelpfulnessNumerator']\n",
    "scaler = StandardScaler()\n",
    "data[['Good', 'Bad']] = scaler.fit_transform(data[['Good', 'Bad']])\n",
    "data.drop(['HelpfulnessDenominator', 'HelpfulnessNumerator'], axis=1, inplace=True)\n",
    "# Calculate TF-IDF for Text and Summary\n",
    "# Initialize TF-IDF vectorizers for Text and Summary with common parameters\n",
    "tfidf_params = {'input': 'content', 'analyzer': 'word', 'stop_words': 'english'}\n",
    "text_vectorizer = TfidfVectorizer(**tfidf_params)\n",
    "summary_vectorizer = TfidfVectorizer(**tfidf_params)\n",
    "\n",
    "# Apply TF-IDF transformation on 'Text' and 'Summary' columns\n",
    "text_matrix = text_vectorizer.fit_transform(data['Text'])\n",
    "summary_matrix = summary_vectorizer.fit_transform(data['Summary'])\n",
    "\n",
    "# Create a sparse matrix for numerical features (Good and Bad)\n",
    "numerical_data = data[['Good', 'Bad']].values\n",
    "numerical_features = scipy.sparse.csr_matrix(numerical_data)\n",
    "\n",
    "# Combine text, summary, numerical, and encoded categorical features into a single feature matrix\n",
    "all_features = scipy.sparse.hstack([\n",
    "    text_matrix,\n",
    "    summary_matrix,\n",
    "    numerical_features,\n",
    "    encoded_product_user_ids\n",
    "])\n",
    "\n",
    "# Separate test and train indices based on missing values in 'Score'\n",
    "mask = data[\"Score\"].isnull()\n",
    "test_indices = np.where(mask)[0]\n",
    "train_indices = np.where(~mask)[0]\n",
    "\n",
    "# Here we apply the scipy method from outside source to reduce compuatation and \n",
    "# to store the one-hot encoded categorical features and TF-IDF vectors\n",
    "train_X = scipy.sparse.csr_matrix(all_features)[train_indices]\n",
    "test_X = scipy.sparse.csr_matrix(all_features)[test_indices]\n",
    "train_Y = data['Score'].loc[~mask].reset_index(drop=True)\n",
    "\n",
    "def CVKFold_Logistic(num_folds, features, labels):\n",
    "    np.random.seed(1)\n",
    "    cross_validator = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
    "    total_accuracy = 0\n",
    "    final_confusion_matrix = None\n",
    "\n",
    "    for fold_index, (train_idx, val_idx) in enumerate(cross_validator.split(features, labels)):\n",
    "        train_features, val_features = features[train_idx], features[val_idx]\n",
    "        train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "\n",
    "        # Logistic Regression Model\n",
    "        logistic_model = LogisticRegression(random_state=0, max_iter=1000)\n",
    "        logistic_model.fit(train_features, train_labels)\n",
    "\n",
    "        # Prediction and Accuracy Calculation for Validation Fold\n",
    "        val_predictions = logistic_model.predict(val_features)\n",
    "        fold_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        total_accuracy += fold_accuracy\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        if final_confusion_matrix is None:\n",
    "            final_confusion_matrix = confusion_matrix(val_labels, val_predictions)\n",
    "        else:\n",
    "            final_confusion_matrix += confusion_matrix(val_labels, val_predictions)\n",
    "\n",
    "        # Extract Important Words\n",
    "        if fold_index == 0:  # Extract once on the first fold\n",
    "            feature_names = text_vectorizer.get_feature_names_out()\n",
    "            coefficients = logistic_model.coef_.flatten()[:len(feature_names)]\n",
    "            \n",
    "            # Combine words and coefficients, sort by influence\n",
    "            important_words_df = pd.DataFrame({'word': feature_names, 'CO': coefficients})\n",
    "            important_words_df = important_words_df.reindex(\n",
    "                important_words_df['CO'].abs().sort_values(ascending=False).index\n",
    "            )\n",
    "            print(\"Top Positive Words:\", important_words_df.head(10))\n",
    "            print(\"Top Negative Words:\", important_words_df.tail(10))\n",
    "\n",
    "    # Calculate average accuracy\n",
    "    average_accuracy = total_accuracy / num_folds\n",
    "    print(\"Average Accuracy:\", average_accuracy)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(final_confusion_matrix / final_confusion_matrix.sum(axis=1)[:, np.newaxis],\n",
    "                annot=True, fmt=\".2f\", cmap=\"magma\", cbar=True)\n",
    "    plt.title(\"Confusion matrix of the classifier\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    return logistic_model\n",
    "\n",
    "# Train Logistic Regression using cross-validation with Stratified K-Fold\n",
    "final_model = CVKFold_Logistic(5, train_X, train_Y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_data = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample.csv')\n",
    "prediction_output = pd.DataFrame(sample_submission)\n",
    "\n",
    "# Predict and save results\n",
    "prediction_output['Score'] = final_model.predict(test_X)\n",
    "prediction_output.to_csv('predicted_scores_logistic.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
